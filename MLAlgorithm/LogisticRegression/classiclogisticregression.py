# -*- coding: utf-8 -*-
"""ClassicLogisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lc8v8ZchJ1Zh2Gc37crFiYu3IpIMVAKb
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
data = pd.read_csv('Classifier1.csv')
X = np.array(data.iloc[:,0:2])
y = np.array(data.iloc[:,2])
print(type(X))
print(type(y))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('Classifier1.csv')
data.head()

def plotData(X,y):
    pos=np.where(y==1)
    neg=np.where(y==0)
    plt.scatter(X[pos,0],X[pos,1],marker='o',c='black')
    plt.scatter(X[neg,0],X[neg,1],marker='x',c='red')
    plt.show()
plotData(X,y)

def sigmoid(X):
    return 1/(1+np.exp(-1*X))

def h(theta,X):
    return np.dot(X,theta)
    
def costfunction(theta,X,y):
    m=len(y)
    j = (-1/m)*np.sum( y * np.log(sigmoid(h(theta,X)))  + (1-y)*np.log(1-sigmoid(h(theta,X))))
    return j

def insert_ones(X):
    X_bias = np.c_[np.ones(X.shape[0]),X]
    return X_bias

def gradient(theta,X,y):
    hypo = sigmoid(h(theta,X))
    grad = np.dot(np.transpose(X),(hypo - y))
    return grad

def predict(theta,X):
    p = np.zeros(shape=(X.shape[0],1))
    p = sigmoid(np.dot(X,theta))
    return p > 0.5

def fit(theta,X,y):
    from scipy import optimize as opt
    weights = opt.fmin_tnc(costfunction,x0=theta,fprime=gradient,args=(X,y.flatten()))
    return weights[0]

X_bias=insert_ones(X)
theta=np.zeros(shape=(X_bias.shape[1],))
#print(theta.shape)
#print(X_bias.shape)
#print(y.shape)
print("Initial Error : ",costfunction(theta,X_bias,y))
parameters=fit(theta,X_bias,y)
print("Learned parameters : ",parameters) 
test_theta = np.array([-24, 0.2, 0.2]).reshape(3,)
err = costfunction(test_theta, X_bias, y)
print("Error in Test environment : ",err)
print("Error with learened parameter : ",costfunction(parameters,X_bias,y))

def map_feature(x1, x2):
    '''
    Maps the two input features to quadratic features.
    Returns a new feature array with more features, comprising of
    X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, etc...
    Inputs X1, X2 must be the same size
    '''
    x1.shape = (x1.size, 1)
    x2.shape = (x2.size, 1)
    degree = 4
    out = np.ones(shape=(x1[:, 0].size, 1))

    m, n = out.shape

    for i in range(1, degree + 1):
        for j in range(i + 1):
            r = (x1 ** (i - j)) * (x2 ** j)
            out = np.append(out, r, axis=1)
            

    return out

predictions = predict(parameters,X_bias)
accuracy = np.mean(predictions == y)
print("Accuracy in percent ",accuracy)